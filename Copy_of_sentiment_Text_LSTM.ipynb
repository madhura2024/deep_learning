{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRWeuJkNredurkseME8g2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhura2024/deep_learning/blob/main/Copy_of_sentiment_Text_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsiRnwKSebn6",
        "outputId": "37d68298-a6c1-4f88-d11e-3273c13df5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 37ms/step - accuracy: 0.8050 - loss: 0.4750\n",
            "Epoch 2/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 35ms/step - accuracy: 0.9460 - loss: 0.1553\n",
            "Epoch 3/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.9745 - loss: 0.0810\n",
            "Epoch 4/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.9850 - loss: 0.0536\n",
            "Epoch 5/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.9892 - loss: 0.0407\n",
            "Epoch 6/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.9895 - loss: 0.0371\n",
            "Epoch 7/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 36ms/step - accuracy: 0.9939 - loss: 0.0227\n",
            "Epoch 8/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 36ms/step - accuracy: 0.9946 - loss: 0.0182\n",
            "Epoch 9/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - accuracy: 0.9961 - loss: 0.0158\n",
            "Epoch 10/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 32ms/step - accuracy: 0.9965 - loss: 0.0122\n",
            "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9136 - loss: 0.3960\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step\n",
            "\n",
            "Message: the flight was delayed and staff were rude\n",
            "Sentiment: Negative\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\n",
            "Message: amazing service and friendly crew\n",
            "Sentiment: Positive\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "read = pd.read_csv('Airline-Sentiment-2-w-AA.csv')\n",
        "\n",
        "read = read[['text', 'airline_sentiment']]\n",
        "read = read[(read['airline_sentiment'] == 'positive') | (read['airline_sentiment'] == 'negative')]\n",
        "read['label'] = read['airline_sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "\n",
        "corpus = read['text'].values\n",
        "labels = read['label'].values\n",
        "\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "cleaned_corpus = []\n",
        "\n",
        "for sentence in corpus:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    sentence = re.sub('\\\\s+', ' ', sentence).strip()\n",
        "\n",
        "    words = sentence.split()\n",
        "    filtered_words = [\n",
        "        wordnet_lemmatizer.lemmatize(w)\n",
        "        for w in words\n",
        "        if w not in stop_words\n",
        "    ]\n",
        "\n",
        "    cleaned_corpus.append(' '.join(filtered_words))\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(cleaned_corpus)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(cleaned_corpus)\n",
        "X = pad_sequences(sequences, maxlen=50, padding='pre')\n",
        "y = np.array(labels).astype('float32')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,output_dim=50,input_length=max_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "# print(f\"Test Loss: {loss:.4f}\")\n",
        "# print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "def predict_sentiment(msg):\n",
        "    msg = msg.lower()\n",
        "    msg = re.sub('[^a-zA-Z]', ' ', msg)\n",
        "    msg = re.sub('\\\\s+', ' ', msg).strip()\n",
        "\n",
        "    words = [\n",
        "        wordnet_lemmatizer.lemmatize(w)\n",
        "        for w in msg.split()\n",
        "        if w not in stop_words\n",
        "    ]\n",
        "\n",
        "    cleaned_msg = ' '.join(words)\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([cleaned_msg])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_len, padding='pre')\n",
        "\n",
        "    pred = model.predict(padded_seq)[0][0]\n",
        "\n",
        "    print(\"\\nMessage:\", msg)\n",
        "    print(\"Sentiment:\", \"Positive\" if pred > 0.5 else \"Negative\")\n",
        "\n",
        "\n",
        "\n",
        "predict_sentiment(\"The flight was delayed and staff were rude\")\n",
        "predict_sentiment(\"Amazing service and friendly crew\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why multiple stemmers were removed\n",
        "\n",
        "Porter, Snowball, and Regexp stemmers all do the same job.\n",
        "\n",
        "Using many stemmers makes code long and confusing.\n",
        "\n",
        "Lemmatization alone is enough for sentiment analysis.\n",
        "\n",
        "So extra stemmers were removed.\n",
        "\n",
        "2. Why Named Entity Recognition (NER) was removed\n",
        "\n",
        "NER finds names of people, places, or organizations.\n",
        "\n",
        "Sentiment analysis only needs positive or negative feeling.\n",
        "\n",
        "\n",
        "3. Why Bag-of-Words (BoW) was removed\n",
        "\n",
        "BoW counts words but does not keep word order.\n",
        "\n",
        "LSTM needs word order to understand meaning.\n",
        "\n",
        "Example:\n",
        "“not good” ≠ “good not”\n",
        "But BoW treats them as same.\n",
        "\n",
        "So BoW is not suitable for LSTM.\n",
        "\n",
        "BoW is better for classical models, not sequence models.\n",
        "\n",
        "4. Why TF-IDF and ANN were removed\n",
        "\n",
        "TF-IDF produces fixed-length vectors.\n",
        "\n",
        "ANN works on fixed-length input, not sequences.\n",
        "\n",
        "LSTM works on sequences, not vectors.\n",
        "\n",
        "Mixing ANN + LSTM in one project causes confusion.\n",
        "\n",
        "So TF-IDF and ANN were removed.\n",
        "\n",
        "5. Why Word2Vec was removed\n",
        "\n",
        "Keras Embedding layer already learns word meanings.\n",
        "\n",
        "Word2Vec is optional, not compulsory.\n",
        "\n",
        "Using both increases complexity.\n",
        "\n",
        "So Word2Vec was removed.\n",
        "\n",
        "6. Why only tokenization + padding is used\n",
        "\n",
        "LSTM needs text in sequence form.\n",
        "\n",
        "Tokenizer converts words into numbers.\n",
        "\n",
        "Padding makes all sequences same length.\n",
        "\n",
        "This is the correct input format for LSTM.\n",
        "\n",
        "7. Why Embedding layer is used\n",
        "\n",
        "Embedding layer converts word numbers into vectors.\n",
        "\n",
        "It learns word meaning during training.\n",
        "\n",
        "No need for external embeddings.\n",
        "\n",
        "This works best with LSTM.\n",
        "\n",
        "8. Why many-to-one LSTM is used\n",
        "\n",
        "Input: many words (sequence)\n",
        "\n",
        "Output: one label (positive or negative)\n",
        "\n",
        "This matches sentiment analysis perfectly.\n",
        "\n",
        "9. BoW + LSTM — why it is a bad idea\n",
        "\n",
        "BoW removes word order.\n",
        "\n",
        "LSTM needs word order.\n",
        "\n",
        "Combining both defeats LSTM’s purpose.\n",
        "\n",
        "Technically possible but not meaningful.\n",
        "\n",
        "10. Correct approach followed in this project\n",
        "\n",
        "Clean text\n",
        "\n",
        "Remove stopwords\n",
        "\n",
        "Lemmatize words\n",
        "\n",
        "Convert text to sequences\n",
        "\n",
        "Pad sequences\n",
        "\n",
        "Use Embedding + LSTM + Sigmoid output\n",
        "\n",
        "Predict Positive or Negative sentiment\n",
        "\n",
        "11. Final conclusion (1-line)\n",
        "\n",
        "BoW is for classical models, LSTM needs sequences, so tokenized sequences with embeddings are used instead of BoW."
      ],
      "metadata": {
        "id": "8lX7CJQ_pW3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "with open(\"alice.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = text.lower()\n",
        "text = re.sub('[^a-zA-Z ]', ' ', text)\n",
        "text = re.sub('\\s+', ' ', text).strip()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "words = tokenizer.texts_to_sequences([text])[0]\n",
        "words = np.array(words)\n",
        "\n",
        "\n",
        "generator = TimeseriesGenerator(data=words,targets=words,length=5,batch_size=128)\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=seq_length))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
        "model.fit(generator, epochs=5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_text(seed_text, next_words, temperature=1.0):\n",
        "    for _ in range(next_words):\n",
        "        seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        seq = pad_sequences([seq], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        preds = model.predict(seq, verbose=0)[0]\n",
        "        preds = np.log(preds + 1e-8) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        next_index = np.random.choice(len(preds), p=preds)\n",
        "        next_word = tokenizer.index_word.get(next_index, '')\n",
        "\n",
        "        seed_text += \" \" + next_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "print(generate_text(\"alice was beginning\", 20, temperature=0.7))\n",
        "print(generate_text(\"alice was beginning\", 20, temperature=1.0))\n",
        "print(generate_text(\"alice was beginning\", 20, temperature=1.2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y2EoxzvCioW",
        "outputId": "49e0f062-39dd-433c-9d74-1bc19679ee33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:20: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:20: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-1454384147.py:20: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  text = re.sub('\\s+', ' ', text).strip()\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.0538 - loss: 6.8646\n",
            "Epoch 2/5\n",
            "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 43ms/step - accuracy: 0.0588 - loss: 5.9132\n",
            "Epoch 3/5\n",
            "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 43ms/step - accuracy: 0.0616 - loss: 5.8189\n",
            "Epoch 4/5\n",
            "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 37ms/step - accuracy: 0.0727 - loss: 5.6294\n",
            "Epoch 5/5\n",
            "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - accuracy: 0.0928 - loss: 5.4490\n",
            "alice was beginning and the minute in the had jury but the white hare it of the caterpillar as the queen only the\n",
            "alice was beginning to be conduct fetch alice could said the from you long way said the house about surprised but i know\n",
            "alice was beginning as they see to see her at one on run the good asking nor interrupt before all begins it s\n"
          ]
        }
      ]
    }
  ]
}