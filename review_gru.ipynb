{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYYH3Yi1rtmQiWU/WidrSm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhura2024/deep_learning/blob/main/review_gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYDYipwjq5sS"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# IMPORTS\n",
        "# ===============================\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# ===============================\n",
        "# LOAD DATASET\n",
        "# ===============================\n",
        "# Dataset columns: content, score (1–5)\n",
        "data = pd.read_csv(\"app_reviews.csv\")\n",
        "\n",
        "x = data['content']\n",
        "y = data['score']\n",
        "\n",
        "# ===============================\n",
        "# TEXT PREPROCESSING\n",
        "# ===============================\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "cleaned_corpus = []\n",
        "\n",
        "for statement in x:\n",
        "    statement = statement.lower()\n",
        "    statement = re.sub('[^a-zA-Z]', ' ', statement)\n",
        "    statement = re.sub('\\\\s+', ' ', statement).strip()\n",
        "\n",
        "    words = word_tokenize(statement)\n",
        "    filtered_words = []\n",
        "\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            filtered_words.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    cleaned_corpus.append(' '.join(filtered_words))\n",
        "\n",
        "# ===============================\n",
        "# TEXTBLOB SENTIMENT ANALYSIS\n",
        "# ===============================\n",
        "data['sentiment_score'] = data['content'].apply(\n",
        "    lambda text: TextBlob(text).sentiment.polarity\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# TOKENIZATION & PADDING\n",
        "# ===============================\n",
        "max_features = 10000\n",
        "max_len = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(cleaned_corpus)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(cleaned_corpus)\n",
        "x_pad = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "# ===============================\n",
        "# ONE HOT ENCODING OF SCORES\n",
        "# ===============================\n",
        "y = y - 1                      # convert 1–5 → 0–4\n",
        "y = to_categorical(y, num_classes=5)\n",
        "\n",
        "# ===============================\n",
        "# TRAIN TEST SPLIT\n",
        "# ===============================\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_pad, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# GRU MODEL\n",
        "# ===============================\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_features, output_dim=128, input_length=max_len))\n",
        "model.add(GRU(64))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# MODEL TRAINING\n",
        "# ===============================\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# MODEL EVALUATION\n",
        "# ===============================\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# ===============================\n",
        "# ACCURACY GRAPH\n",
        "# ===============================\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ===============================\n",
        "# USER INPUT PREDICTION\n",
        "# ===============================\n",
        "def predict_review():\n",
        "    review = input(\"\\nEnter app review: \")\n",
        "\n",
        "    review_clean = review.lower()\n",
        "    review_clean = re.sub('[^a-zA-Z]', ' ', review_clean)\n",
        "    review_clean = re.sub('\\\\s+', ' ', review_clean).strip()\n",
        "\n",
        "    words = [\n",
        "        lemmatizer.lemmatize(w)\n",
        "        for w in review_clean.split()\n",
        "        if w not in stop_words\n",
        "    ]\n",
        "\n",
        "    cleaned_text = \" \".join(words)\n",
        "\n",
        "    seq = tokenizer.texts_to_sequences([cleaned_text])\n",
        "    pad = pad_sequences(seq, maxlen=max_len, padding='pre')\n",
        "\n",
        "    prediction = model.predict(pad)\n",
        "    rating = np.argmax(prediction) + 1\n",
        "\n",
        "    sentiment = TextBlob(review).sentiment.polarity\n",
        "\n",
        "    print(\"\\nOriginal Review:\", review)\n",
        "    print(\"Predicted Rating (1–5):\", rating)\n",
        "    print(\"TextBlob Sentiment Score:\", sentiment)\n",
        "\n",
        "# ===============================\n",
        "# TEST WITH USER INPUT\n",
        "# ===============================\n",
        "predict_review()\n",
        "predict_review()\n"
      ]
    }
  ]
}