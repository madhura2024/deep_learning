{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgPflLbImzYa1X8YywfKEX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhura2024/deep_learning/blob/main/summarization_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FXBGo-K82SW",
        "outputId": "0febb9c4-ae40-4276-df21-3fa23bded078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:73: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:73: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-3597422677.py:73: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  s = re.sub('\\s+', ' ', s).strip()\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Enter text to summarize:\n",
            "Ilya glanced back, dark eyes flashing just for Shane. “He gets that from you.”  “Obviously,” Shane deadpanned. “All the best parts.”  It smelled of yeast, honey, and butter. Anya snored under the table, defeated after three hopeful sniffs at fallen crumbs. Outside, a few yellow leaves scraped along the deck. The weather report promised rain tonight.  Ilya glanced at the recipe on his phone. “Mom used to sing while she cooked,” he mumbled. “We had an old junky TV in the kitchen corner. The remote didn’t work half the time, but Mom would keep it tuned to one of those music channels. She’d hum along… sometimes sing, if she liked the song enough. Ahh—” He laughed under his breath. “I wish I remembered the words now.”  Shane said nothing at first. He reached over, brushed flour from Ilya’s knuckles, thumb lingering where the pulse beat steady and warm. “I think you are a lot like her, you know.”  Ilya’s head snapped up, startled, eyes meeting Shane’s. His brows knit together before he shook his head. “No,” he said, almost defensive.  Shane’s gaze stayed soft. “Yeah. The way you look after people. The way you make any room feel safe and sound. She’d love how you turned out.”  A faint smile ghosted over Ilya’s lips, reluctant but there. “She’d probably scold me for feeding Archie too much candy.”  Shane’s grin flashed bright and boyish. “Then she and I would’ve been best friends.”\n",
            "\n",
            "ORIGINAL TEXT:\n",
            "\n",
            "Ilya glanced back, dark eyes flashing just for Shane. “He gets that from you.”  “Obviously,” Shane deadpanned. “All the best parts.”  It smelled of yeast, honey, and butter. Anya snored under the table, defeated after three hopeful sniffs at fallen crumbs. Outside, a few yellow leaves scraped along the deck. The weather report promised rain tonight.  Ilya glanced at the recipe on his phone. “Mom used to sing while she cooked,” he mumbled. “We had an old junky TV in the kitchen corner. The remote didn’t work half the time, but Mom would keep it tuned to one of those music channels. She’d hum along… sometimes sing, if she liked the song enough. Ahh—” He laughed under his breath. “I wish I remembered the words now.”  Shane said nothing at first. He reached over, brushed flour from Ilya’s knuckles, thumb lingering where the pulse beat steady and warm. “I think you are a lot like her, you know.”  Ilya’s head snapped up, startled, eyes meeting Shane’s. His brows knit together before he shook his head. “No,” he said, almost defensive.  Shane’s gaze stayed soft. “Yeah. The way you look after people. The way you make any room feel safe and sound. She’d love how you turned out.”  A faint smile ghosted over Ilya’s lips, reluctant but there. “She’d probably scold me for feeding Archie too much candy.”  Shane’s grin flashed bright and boyish. “Then she and I would’ve been best friends.”\n",
            "\n",
            "SUMMARY:\n",
            "\n",
            "The remote didn’t work half the time, but Mom would keep it tuned to one of those music channels.\n",
            "He reached over, brushed flour from Ilya’s knuckles, thumb lingering where the pulse beat steady and warm.\n",
            "“She’d probably scold me for feeding Archie too much candy.”  Shane’s grin flashed bright and boyish.\n",
            "“I think you are a lot like her, you know.”  Ilya’s head snapped up, startled, eyes meeting Shane’s.\n",
            "Anya snored under the table, defeated after three hopeful sniffs at fallen crumbs.\n",
            "She’d love how you turned out.”  A faint smile ghosted over Ilya’s lips, reluctant but there.\n",
            "She’d hum along… sometimes sing, if she liked the song enough.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "!pip install gensim\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
        "from nltk.stem import PorterStemmer, RegexpStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "def summarize_text(text, summary_ratio=0.3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    for s in sentences:\n",
        "        word_tokenize(s)\n",
        "        WordPunctTokenizer().tokenize(s)\n",
        "    porterstem = PorterStemmer()\n",
        "    snowball = SnowballStemmer('english')\n",
        "    regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for s in sentences:\n",
        "        for w in word_tokenize(s.lower()):\n",
        "            porterstem.stem(w)\n",
        "            snowball.stem(w)\n",
        "            regexp.stem(w)\n",
        "            lemmatizer.lemmatize(w, pos='v')\n",
        "            lemmatizer.lemmatize(w, pos='n')\n",
        "\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_sentences = []\n",
        "\n",
        "    for s in sentences:\n",
        "        s = s.lower()\n",
        "        s = re.sub('[^a-zA-Z]', ' ', s)\n",
        "        s = re.sub('\\s+', ' ', s).strip()\n",
        "\n",
        "        words = []\n",
        "        for w in s.split():\n",
        "            if w not in stop_words:\n",
        "                words.append(lemmatizer.lemmatize(w))\n",
        "\n",
        "        cleaned_sentences.append(\" \".join(words))\n",
        "\n",
        "\n",
        "    for s in cleaned_sentences:\n",
        "        ne_chunk(pos_tag(word_tokenize(s)))\n",
        "\n",
        "\n",
        "    cv = CountVectorizer()\n",
        "    bow = cv.fit_transform(cleaned_sentences)\n",
        "\n",
        "    ngram = CountVectorizer(ngram_range=(1,2))\n",
        "    ngram_bow = ngram.fit_transform(cleaned_sentences)\n",
        "\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf.fit_transform(cleaned_sentences)\n",
        "\n",
        "    word2vec = Word2Vec(\n",
        "        sentences=[s.split() for s in cleaned_sentences],\n",
        "        vector_size=50,\n",
        "        window=3,\n",
        "        min_count=1\n",
        "    )\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for i in range(len(sentences)):\n",
        "        sentence_scores[sentences[i]] = np.sum(tfidf_matrix[i].toarray())\n",
        "\n",
        "    summary_len = max(1, int(len(sentences) * summary_ratio))\n",
        "    ranked_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "    summary = ranked_sentences[:summary_len]\n",
        "\n",
        "    print(\"\\nORIGINAL TEXT:\\n\")\n",
        "    print(text)\n",
        "\n",
        "    print(\"\\nSUMMARY:\\n\")\n",
        "    for s in summary:\n",
        "        print(s)\n",
        "\n",
        "user_text = input(\"Enter text to summarize:\\n\")\n",
        "summarize_text(user_text)\n"
      ]
    }
  ]
}